var documenterSearchIndex = {"docs":
[{"location":"api/","page":"API","title":"API","text":"Modules = [QuantumPrisonersDilemmaModel]\nOrder   = [:type, :function]\nPrivate = false","category":"page"},{"location":"api/#QuantumPrisonersDilemmaModel.QPDM","page":"API","title":"QuantumPrisonersDilemmaModel.QPDM","text":"QPDM{T<:Real} <: AbstractQPDM\n\nA model object for the Quantum Prisoner's Dilemma Model. The QPDM has four basis states:\n\nopponent defects and you defect \nopponent defects and you cooperate \nopponent cooperates and you defect \nopponent cooperates and you cooperate\n\nThe bases are orthonormal and in standard form. The model assumes three conditions:\n\nPlayer 2 is told that player 1 defected\nPlayer 2 is told that player 1 cooperated\nPlayer 2 is not informed of the action of player 1\n\nModel inputs and outputs are assumed to be in the order above. \n\nFields\n\nμd: utility for defecting \nμc: utility for cooperating \nγ: entanglement parameter for beliefs and actions \n\nExample\n\nusing QuantumPrisonersDilemmaModel\nmodel = QPDM(;μd=.51, γ=2.09)\n\nReferences\n\nPothos, E. M., & Busemeyer, J. R. (2009). A quantum probability explanation for violations of ‘rational’decision theory. Proceedings of the Royal Society B: Biological Sciences, 276(1665), 2171-2178.\n\n\n\n\n\n","category":"type"},{"location":"api/#Base.rand-Tuple{QuantumPrisonersDilemmaModel.AbstractQPDM, Int64}","page":"API","title":"Base.rand","text":"rand(dist::AbstractQPDM, n::Int; t = π / 2)\n\nGenerates simulated data for the following conditions:\n\nPlayer 2 is told that player 1 defected\nPlayer 2 is told that player 1 cooperated\nPlayer 2 is not informed of player 1's action\n\nArguments\n\ndist::AbstractQPDM\nn: the number of trials per condition \n\nKeywords\n\nt = π / 2: time of decision\n\nExample\n\nusing QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\ndata = rand(model, 100)\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{QuantumPrisonersDilemmaModel.AbstractQPDM, Int64, Vector{Int64}}","page":"API","title":"Distributions.logpdf","text":"logpdf(dist::AbstractQPDM, n::Int, n_d::Vector{Int}; t = π / 2)\n\nReturns the joint log density given data for the following conditions:\n\nPlayer 2 is told that player 1 defected\nPlayer 2 is told that player 1 cooperated\nPlayer 2 is not informed of player 1's action\n\nArguments\n\ndist::AbstractQPDM\nn: the number of trials per condition \nn_d: the number of defections in each condition \n\nKeywords\n\nt = π / 2: time of decision\n\nExample\n\nusing QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\nn_trials = 100\ndata = rand(model, n_trials)\nlogpdf(model, n_trials, data)\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.pdf-Tuple{QuantumPrisonersDilemmaModel.AbstractQPDM, Int64, Vector{Int64}}","page":"API","title":"Distributions.pdf","text":"pdf(dist::AbstractQPDM, n::Int, n_d::Vector{Int}; t = π / 2)\n\nReturns the joint probability density given data for the following conditions:\n\nPlayer 2 is told that player 1 defected\nPlayer 2 is told that player 1 cooperated\nPlayer 2 is not informed of player 1's action\n\nArguments\n\ndist::AbstractQPDM\nn: the number of trials per condition \nn_d: the number of defections in each condition \n\nKeywords\n\nt = π / 2: time of decision\n\n\n\n\n\n","category":"method"},{"location":"api/#QuantumPrisonersDilemmaModel.predict-Tuple{QuantumPrisonersDilemmaModel.AbstractQPDM}","page":"API","title":"QuantumPrisonersDilemmaModel.predict","text":"predict(dist::AbstractQPDM; t = π / 2)\n\nReturns predicted response probability for the following conditions:\n\nPlayer 2 is told that player 1 defected\nPlayer 2 is told that player 1 cooperated\nPlayer 2 is not informed of player 1's action\n\nArguments\n\ndist::AbstractQPDM\n\nKeywords\n\nt = π / 2: time of decision\n\nExample\n\nusing QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\npredict(model)\n\n\n\n\n\n","category":"method"},{"location":"parameter_estimation/#Parameter-Estimation","page":"Parameter Estimation","title":"Parameter Estimation","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"This brief tutorial explains how to performance Bayesian parameter estimation of the QPDM using Pigeons.jl. One complication in estimating the parameters of the QPDM is that the posterior distributions may have multiple modes, which leads to convergence problems with most MCMC algorithms. Pigeons.jl uses a special type of parallel tempering to overcome this challenge. An additional advantage of using Pigeons.jl is the ability to compute Bayes factors from the log marginal likelihood using the function stepping_stone.","category":"page"},{"location":"parameter_estimation/#Load-Packages","page":"Parameter Estimation","title":"Load Packages","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"First, we will load the required packages below. ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"using Pigeons\nusing QuantumPrisonersDilemmaModel\nusing Random\nusing StatsPlots\nusing Turing","category":"page"},{"location":"parameter_estimation/#Generate-Simulated-Data","page":"Parameter Estimation","title":"Generate Simulated Data","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The next step is to generate some simulated data from which the parameters can be estimated. In the code block below, the utility parameter mu_d is set to one and the entanglement parameter is set to gamma = 2.  A total of 50 trials is generated for each of the three conditions. The resulting values represent the number of defections per condition out of 50.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Random.seed!(65)\nn = 50\nparms = (μd=1.0, γ=2.0)\nmodel = QPDM(;parms...)\ndata = rand(model, n)","category":"page"},{"location":"parameter_estimation/#Define-Turing-Model","page":"Parameter Estimation","title":"Define Turing Model","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The next step is to define a Turing model with the @model macro. For simplicity, we will fix the utility parameter mu_d=1 and set the prior of the entanglement parameter to gamma sim mathrmnormal(03). ","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"@model function turing_model(data, parms)\n    γ ~ Normal(0, 3)\n    data ~ QPDM(;parms..., γ)\nend","category":"page"},{"location":"parameter_estimation/#Estimate-Parameters","page":"Parameter Estimation","title":"Estimate Parameters","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"To estimate the parameters, we need to pass the Turing model to pigeons. The second command converts the output to an MCMCChain object, which can be used for plotting","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"pt = pigeons(\n    target=TuringLogPotential(turing_model((n, data), parms)), \n    record=[traces])\nsamples = Chains(sample_array(pt), [\"γ\"])","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"The trace of the pigeon's sampler is given below:","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"────────────────────────────────────────────────────────────────────────────\n  scans        Λ      log(Z₁/Z₀)   min(α)     mean(α)    min(αₑ)   mean(αₑ) \n────────── ────────── ────────── ────────── ────────── ────────── ──────────\n        2       1.58      -11.3      0.332      0.825          1          1 \n        4      0.529      -10.1      0.522      0.941          1          1 \n        8       1.11      -9.43      0.501      0.877          1          1 \n       16       1.37      -9.89       0.66      0.847          1          1 \n       32       1.48      -10.3      0.772      0.836          1          1 \n       64       1.46      -10.1      0.735      0.837          1          1 \n      128       1.44      -10.4      0.776       0.84          1          1 \n      256       1.49      -10.4      0.772      0.834      0.999          1 \n      512       1.46      -10.3      0.816      0.838      0.999          1 \n 1.02e+03       1.48      -10.3      0.817      0.836      0.999          1 \n────────────────────────────────────────────────────────────────────────────","category":"page"},{"location":"parameter_estimation/#Plot-Posterior-Distribution","page":"Parameter Estimation","title":"Plot Posterior Distribution","text":"","category":"section"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"Now we can plot the posterior distribution of gamma with plot. The posterior distribution of gamma has a primary mode around 1 and secondary modes around 2 and 3.5.","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"plot(samples)","category":"page"},{"location":"parameter_estimation/","page":"Parameter Estimation","title":"Parameter Estimation","text":"(Image: )","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"warning: Warning\nThis page is under construction.","category":"page"},{"location":"model_description/#Introduction","page":"Model Description","title":"Introduction","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"In this tutorial, we present a quantum cognition model of interference effect's in the prisoner's dilemma. The goal is to provide a conceptual understanding of the model and how it produces interference effects. Before introducing the model, we will briefly describe the Prisoner's dilemma and the interference effect.  ","category":"page"},{"location":"model_description/#The-Prisoner's-Dilemma","page":"Model Description","title":"The Prisoner's Dilemma","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The prisoner's dilemma (PD) is a two person interdependent decision making task from game theory. In a typical version of the PD, two players decide simultanouesly (i.e., without knowing the decision of the other) to cooperate or defect. The payoff each player recieves depends on both his or her choice and the choice of the other player. A typical payoff matrix is presented in the table below where entries (x_1x_2) correspond to the payoff given to player 1 and player 2, respectively. Assuming rational, self-interested players, the Nash equalibrium occurs when both players defect. Even though cooperating would lead to a better collective outcome (20 vs. 10), each individual has the incentive to defect. For example, if player 2 cooperates, player 1 recieves a better payoff by defecting (25 vs. 20). Alternatively, if player 2 defects, player 1 also recieves a better payoff by defecting (10 vs. 5).  ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<style type=\"text/css\">\n.tg  {border-collapse:collapse;border-spacing:0;}\n.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;\n  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}\n.tg .tg-0lax{text-align:left;vertical-align:top}\n</style>\n<table class=\"tg\">\n<thead>\n  <tr>\n    <th class=\"tg-0lax\" colspan=\"2\" rowspan=\"2\"></th>\n    <th class=\"tg-0lax\" colspan=\"2\">Player 2 (you)</th>\n  </tr>\n  <tr>\n    <th class=\"tg-0lax\">Defect</th>\n    <th class=\"tg-0lax\">Cooperate</th>\n  </tr>\n</thead>\n<tbody>\n  <tr>\n    <td class=\"tg-0lax\" rowspan=\"2\">Player 1</td>\n    <td class=\"tg-0lax\">Defect</td>\n    <td class=\"tg-0lax\">(10,10)</td>\n    <td class=\"tg-0lax\">(25,5)</td>\n  </tr>\n  <tr>\n    <td class=\"tg-0lax\">Cooperate</td>\n    <td class=\"tg-0lax\">(5,25)</td>\n    <td class=\"tg-0lax\">(20,20)</td>\n  </tr>\n</tbody>\n</table>","category":"page"},{"location":"model_description/#Interference-Effect","page":"Model Description","title":"Interference Effect","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The interference effect occurs when decisions in the PD violate the law of total probability. Consider an experiment of the prisoner's dilemma with three conditions:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Player 2 is told that player 1 defected: R_2=d\nPlayer 2 is told that player 1 cooperated: R_2=c\nPlayer 2 is not informed of player's action","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The law of total probability requires:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pr(R_2=d) = Pr(R_2=d mid R_1=d) Pr(R_1=d) + Pr(R_2=d mid R_1=c) Pr(R_1=c)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"where the left hand side corresponds to condition 3 and the terms on the right hand side correspond to conditions 1 and 2, respectively. When this inequality does not hold, an interference effect occurs. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"An important property of the law of total probability is that it requires condition 3 to be a weighted average of conditions 1 and 2. This implies the following ordering of terms:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"min(Pr(R_2=d mid R_1=d) Pr(R_2=d mid R_1=c)) leq Pr(R_2=d) leq max(Pr(R_2=d mid R_2=1) Pr(R_2=d mid R_1=c))","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"which is violated in human decision making, leading to an interference effect. ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"An interference effect is present in the data because the response probability in condition 3 is below the response probabilities in conditions 1 and 2. The predictions of the quantum model are shown in the last column.","category":"page"},{"location":"model_description/#Quantum-Model","page":"Model Description","title":"Quantum Model","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"quantum prisoner's dilemma model (QPDM)","category":"page"},{"location":"model_description/#Bases","page":"Model Description","title":"Bases","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The QPDM consists of four basis states respesenting the four possible outcomes for defect (D) and cooperate (C):","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfB = kettextrmDDkettextrmDCkettextrmCDkettextrmCC","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"where basis ketij repesents the outcome in which the opponent chooses i and you choose j. We will assume mathbfB is the standard basis, meaning each basis vector consists of a 1 with all other elements equal to zero, e.g., ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"kettextrmDD = beginbmatrix\n\t1  \n\t0  \n\t0  \n\t0 \nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Combining the basis vectors into a single matrix, we get the identity matrix:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfI_4 = beginbmatrix\t\t\n\t1  0  0  0\n\t0  1  0  0\n\t0  0  1  0\n\t0  0  0  1\nendbmatrix","category":"page"},{"location":"model_description/#States","page":"Model Description","title":"States","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The state of the cognitive system is a superposition (i.e. linear combination) over basis states:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"ketboldsymbolpsi = alpha_textrmDD kettextrmDD+ alpha_textrmDC kettextrmDC+ alpha_textrmCD kettextrmCD+ alpha_textrmCC kettextrmCC","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"where lVertketboldsymbolpsi rVert = 1. The coefficients can be written as:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"boldsymbolalpha = beginbmatrix\n\talpha_textrmDD  \n\talpha_textrmDC  \n\talpha_textrmCD  \n\talpha_textrmCC  \nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The QPDM assumes the initial state of the decision maker is a uniform superposition over basis states: ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"ketboldsymbolpsi_0 = frac12beginbmatrix\n\t1  \n\t1  \n\t1  \n\t1  \nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"In conditions 1 and 2, the state is updated upon learning the decision of player 1: ketboldsymbolpsi_0 rightarrow ketboldsymbolpsi_kk in cd. When player 2 is told that player 1 defected, the state vector becomes:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"ketboldsymbolpsi_d = frac1sqrt2beginbmatrix\n\t1  \n\t1  \n\t0  \n\t0  \nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"indicating that the state collapsed onto the sub-space representing player 1 defected. Similarly, when player 2 is told that player 1 cooperated, the state collapses to the sub-space represnting the event player 1 cooperated, giving: ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"ketboldsymbolpsi_c = frac1sqrt2beginbmatrix\n\t0  \n\t0  \n\t1  \n\t1  \nendbmatrix","category":"page"},{"location":"model_description/#Hamiltonian-Matrices","page":"Model Description","title":"Hamiltonian Matrices","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Hamiltonian matrices govern the decision dynamics of the model. The Hamiltonian matrix mathbfH consists of two components: mathbfH_A is sensitive to the payoff matrix, and mathbfH_B is sensitive to cognitive dissonance between beliefs and actions. The component mathbfH_A is defined as follows: ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfH_A = beginbmatrix\t\t\n\tmathbfH_A_d  mathbf0\n\tmathbf0  mathbfH_A_c\nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"where","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfH_A_k = frac1sqrt1 + mu_k^2beginbmatrix\t\t\n\tmu_k  1\n\t1  -mu_k\nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<details>\n<summary><b>Show Code</b></summary>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"using Plots \nprob_defect(μ, t) = .5 + (μ / (1 + μ^2)) * sin((π * t) / 2)^2\nμs = range(-1, 1, length=5)\nts = range(0, π / 2, length=100)\nplot_Ha = plot(ts, map(μ -> prob_defect.(μ, ts), μs), grid=false, label=μs', \n\txlabel=\"Time\", ylabel=\"Probability Defect\", legendtitle=\"μ\")\nsavefig(\"h1_prob.png\")","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"</details>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"(Image: )","category":"page"},{"location":"model_description/#Action-Selection","page":"Model Description","title":"Action Selection","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"This selection describes the process of selecting an action and determining the defection probability. The time evolution is governed by the unitary transformation matrix which is given by:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfU = e^-i cdot t cdot  mathbfH","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The QPDM makes the simplifying assumption that the decision is made after a relative short period of time where the wave function is at the maximum amplitude: t=fracpi2. Next, we define a projection matrix for computing the probability of defecting. The probability of defecting is the sum of outer products for DD and DC:  ","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"mathbfP = kettextrmDD bratextrmDD + kettextrmDC bratextrmDC = beginbmatrix\t\t\n\t1  0  0  0\n\t0  0  0  0\n\t0  0  1  0\n\t0  0  0  0\nendbmatrix","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The probability of player 2 defecting is given by the squared magnitude of the projection from the current state boldsymbolpsi_1 rightarrow boldsymbolpsi_2. The probability that player 2 defects given that player 1 defected is:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pr(R_2=d mid R_1=d) = lVertmathbfP cdot mathbfU cdot  ketpsi_drVert^2","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The probability that player 2 defects given that player 1 cooperated is:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pr(R_2=d mid R_1=c) = lVertmathbfP cdot  mathbfU cdot  ketpsi_crVert^2","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The probability that player 2 defects given that the action of player 1 is unknown is:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pr(R_2=d) = lVertmathbfP cdot  mathbfU cdot  ketpsi_0rVert^2","category":"page"},{"location":"model_description/#QPDM-Predictions","page":"Model Description","title":"QPDM Predictions","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"As a simple example, the code block illustrates how to generate the predictions in the table above. The first parameter mu_d is the utility for defecting. When no value is passed for the utility of cooperating, mu_d = mu_c. The parameter gamma is the entanglement parameter which aligns beliefs about the opponents action and one's own action. The model predictions are given below, along with the emprical data:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Condition Formula Data Model\n1 Pr(R_1=d mid R_2=d) .84 .81\n2 Pr(R_1=d mid R_2=c) .66 .65\n3 Pr(R_1=d) .55 .57","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The code used to generate the predictions can be viewed by expanding the code block below:","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<details>\n<summary><b>Show Code</b></summary>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"using QuantumPrisonersDilemmaModel\nmodel = QPDM(;μd=.51, γ=2.09)\npreds = predict(model)","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"</details>","category":"page"},{"location":"model_description/#Dynamics","page":"Model Description","title":"Dynamics","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The plot below shows the dynamics of the model for each condition.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<details>\n<summary><b>Show Code</b></summary>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"using Plots\nusing QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\nts = range(0, 3, length=300)\npreds = map(t -> predict(model; t), ts)\ncolor = [RGB(.251,.388,.847) RGB(.584,.345,.689) RGB(.796,.235,.2)]\np1 = plot(ts, reduce(vcat, transpose.(preds)), grid=false, \n    label=[\"p1 Defects\" \"p1 Cooperates\" \"p1 Unknown\"], \n    xlabel=\"Time\", ylabel=\"Prob p2 Defects\", linewidth=2; color)\nsavefig(\"probs_time.png\")","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"</details>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"(Image: )","category":"page"},{"location":"model_description/#Interference-Effects","page":"Model Description","title":"Interference Effects","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"The plot below shows the interference effect as a function of mu for multiple values of gamma. In the simulations below, we fix t=fracpi2.","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"<details>\n<summary><b>Show Code</b></summary>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"using Plots \nusing QuantumPrisonersDilemmaModel \n\nfunction compute_IE(;μ, γ) \n\tpreds = predict(QPDM(;μd=μ, γ)) \n\tif preds[3] < minimum(preds[1:2])\n\t\treturn preds[3] - minimum(preds[1:2])\n\telseif preds[3] > maximum(preds[1:2]) \n\t\treturn preds[3] - maximum(preds[1:2])\n\tend\n\treturn 0.0\nend\n\nmap_mu(;μs, γ) = map(μ -> compute_IE(;μ, γ), μs)\n\ncompute_IE(;μ=.5, γ=2) \nμs = range(-1, 1, length=200)\nγs = range(-2, 2, length=5)\n\nIEs = map(γ -> map_mu(;μs, γ), γs)\n\nie_plot = plot(μs, IEs, grid=false, label=γs', \n\txlabel=\"μ\", ylabel=\"Interference Effect\", legendtitle=\"γ\", legend=:outerright)\nsavefig(\"ie_plot.png\")","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"</details>","category":"page"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"(Image: )","category":"page"},{"location":"model_description/#References","page":"Model Description","title":"References","text":"","category":"section"},{"location":"model_description/","page":"Model Description","title":"Model Description","text":"Pothos, E. M., & Busemeyer, J. R. (2009). A quantum probability explanation for violations of ‘rational’decision theory. Proceedings of the Royal Society B: Biological Sciences, 276(1665), 2171-2178.","category":"page"},{"location":"basic_usage/#Overview","page":"Basic Usage","title":"Overview","text":"","category":"section"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"This page provides an overview of the API along with examples. ","category":"page"},{"location":"basic_usage/#Make-Predictions","page":"Basic Usage","title":"Make Predictions","text":"","category":"section"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"The quantum prisoner's dilemma model (QPDM) generates predictions for three conditions:","category":"page"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"Player 1 is told that player 2 defected\nPlayer 1 is told that player 2 cooperated\nPlayer 1 is not informed of the action of player 2","category":"page"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"using QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\npredict(model)","category":"page"},{"location":"basic_usage/#Simulate-Model","page":"Basic Usage","title":"Simulate Model","text":"","category":"section"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"The code block below demonstrates how to generate simulated data from the model using rand. In the example, we will generate 100 simulated trials for each condition. ","category":"page"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"using QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\ndata = rand(model, 100)","category":"page"},{"location":"basic_usage/#Evaluate-Log-Likelihood","page":"Basic Usage","title":"Evaluate Log Likelihood","text":"","category":"section"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"The log likelihood of data can be evaluated using logpdf. In the code block below, we generate simulated data and evaluate the logpdf: ","category":"page"},{"location":"basic_usage/","page":"Basic Usage","title":"Basic Usage","text":"using QuantumPrisonersDilemmaModel \nmodel = QPDM(;μd=.51, γ=2.09)\nn_trials = 100\ndata = rand(model, n_trials)\nlogpdf(model, n_trials, data)","category":"page"},{"location":"#QuantumPrisonersDilemmaModel.jl","page":"Home","title":"QuantumPrisonersDilemmaModel.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package contains code for a quantum cognition model of inteference effects in the prisoner's dilemma. ","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is not registered in Julia's general registry. However, there two ways you can install the package. Option 1 is to install without version control. In the REPL, use ] to switch to the package mode and enter the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"add https://github.com/itsdfish/QuantumPrisonersDilemmaModel.jl","category":"page"},{"location":"","page":"Home","title":"Home","text":"Option 2 is to install via a custom registry. The advantage of this approach is that you have more control over version control, expecially if you are using a project-specfic environment. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"Install the registry using the directions found here.\nAdd the package by typing ] into the REPL and then typing (or pasting):","category":"page"},{"location":"","page":"Home","title":"Home","text":"add QuantumPrisonersDilemmaModel","category":"page"},{"location":"#References","page":"Home","title":"References","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Pothos, E. M., & Busemeyer, J. R. (2009). A quantum probability explanation for violations of ‘rational’decision theory. Proceedings of the Royal Society B: Biological Sciences, 276(1665), 2171-2178.","category":"page"}]
}
